<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>尽管扯淡-梯度下降从放弃到入门</title>
    <meta name="description" content="资深码农，程序设计语言控">
    <link rel="canonical" href="http://localhost:4000/2017/10/22/gradient-descent-derivation.html">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="stylesheet" href="/assets/stylesheets/main.css">
</head>

<body>

<header class="navbar navbar-inverse navbar-fixed-top" role="banner" data-include="/assets/tpls/header.tpl"></header>
<div class="container">
    <div class="col-md-9">
    <div class="post">

        <header class="post-header">
            <h1 class="post-title">梯度下降从放弃到入门</h1>
            <p class="post-meta">
                2017年10月22日 • 詹子知(James Zhan) • 版权所有，转载须声明出处
            </p>
        </header>

        <article class="post-content">
            <h3 id="section">梯度</h3>

<p>在向量微积分中，标量场的梯度是一个向量场。标量场中某一点上的梯度指向标量场增长最快的方向，梯度的长度是这个最大的变化率。</p>

<p>在三维直角坐标系中表示为：</p>

<script type="math/tex; mode=display">\nabla \varphi = \left( \frac{\partial \varphi}{\partial x}, \frac{\partial \varphi}{\partial y}, \frac{\partial \varphi}{\partial z}\right) 
= \frac{\partial \varphi}{\partial x}\vec{i} + \frac{\partial \varphi}{\partial y}\vec{j} + \frac{\partial \varphi}{\partial z}\vec{k}</script>

<h3 id="section-1">梯度下降</h3>

<p>梯度下降是一个用来求函数最小值的算法，在机器学习所有优化算法当中，梯度下降应该算是最受关注并且用的最多的一个算法。</p>

<p>梯度下降背后的思想是：开始时我们随机选取一个参数的组合$(\theta_0,\theta_1,\cdots,\theta_n)$，计算代价函数$J(\theta)$，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做，直到找到一个局部最小值，因为我们并没有尝试完所有的参数组合，所以不能确定我们得到局部最小值是否便是全局最小值，不同的初始参数组合，可能会找到不同的局部最小值。如果我们知道代价函数是一个凸函数，那么我们就可以保证我们找到的局部最小点就是全局最小点。</p>

<p>通过梯度的定义，我们知道，梯度的方向是增长（上升）最快的方向，那么梯度的反方向便是让代价函数下降程度最大的方向。</p>

<p>定义$\alpha$为学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向更新的步长。</p>

<script type="math/tex; mode=display">\theta_j = \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}</script>

<p>每走一步，我们都要重新计算函数在当前点的梯度，然后选择梯度的反方向作为走下去的方向。随着每一步迭代，梯度不断地减小，到最后减小为零。</p>

<h3 id="section-2">线性回归</h3>

<p>假定我们有一批数据点，需要根据这些点找出最适合的线性方程$y = ax + b$。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.199</span><span class="p">,</span> <span class="mf">0.389</span><span class="p">,</span> <span class="mf">0.580</span><span class="p">,</span> <span class="mf">0.783</span><span class="p">,</span> <span class="mf">0.980</span><span class="p">,</span> <span class="mf">1.177</span><span class="p">,</span> <span class="mf">1.380</span><span class="p">,</span> <span class="mf">1.575</span><span class="p">,</span> <span class="mf">1.771</span><span class="p">]</span>
</code></pre>
</div>

<p><img src="/assets/images/gd_points.png" alt="GDPoints" /></p>

<p>也就是要找出最合适的参数$(a, b)$，使得直线到所有点的距离尽可能小。他可以转化为求解代价函数1</p>

<script type="math/tex; mode=display">J(a, b) = \sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)^2</script>

<p>的最小值。</p>

<p>我们也可以令$J(a, b)$为误差平方和（SSE）的平均数，为了后续求导计算方便，我们还可以再把他除以2，即代价函数2</p>

<script type="math/tex; mode=display">J(a, b) = \frac{1}{2m}\sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)^2</script>

<p>如果$(a, b)$在代价函数2上取得最小值，也必在代价函数1上取的最小值。</p>

<p>其中<script type="math/tex">x^{(i)}, y^{(i)}</script>为已知数。</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial a}J(a,b)
=  \frac{\partial}{\partial a}\left(\frac{1}{2m}\sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)^2\right)
= \frac{1}{m}\sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)x^{(i)}</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial b}J(a,b)
=  \frac{\partial}{\partial b}\left(\frac{1}{2m}\sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)^2\right)
= \frac{1}{m}\sum_{i=1}^m\left(ax^{(i)} + b - y^{(i)}\right)</script>

<p>利用梯度下降法，我们很容易实现对应的迭代解法。</p>

<script type="math/tex; mode=display">a := a - \alpha \frac{\partial}{\partial a}J(a,b)</script>

<script type="math/tex; mode=display">b := b - \alpha \frac{\partial}{\partial b}J(a,b)</script>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">sse0</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">grad_a</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span>
            <span class="n">grad_b</span> <span class="o">+=</span> <span class="n">diff</span>

        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_a</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">/</span> <span class="n">m</span>

        <span class="n">a</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_a</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b</span>

        <span class="n">sse1</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">sse1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sse0</span> <span class="o">-</span> <span class="n">sse1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sse0</span> <span class="o">=</span> <span class="n">sse1</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</code></pre>
</div>

<p>测试代码如下:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>

<span class="k">print</span> <span class="s">'y = {0} * x + {1}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c"># y = 0.193958973089 * x + 0.0161212366801</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Original data'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Fitted line'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>
<p><img src="/assets/images/gd_points_fit.png" alt="GDPointsFit" /></p>

<h3 id="section-3">多变量线性回归</h3>

<p>上例，我们演示了一元线性回归梯度下降的迭代解法，更一般地，我们考虑n个变量的情况，我们有m条记录。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
x_{10} & x_{11} & x_{12} & \cdots & x_{1n} \\
x_{20} & x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{m0} & x_{m1} & x_{m2} & \cdots & x_{mn} \\
\end{bmatrix} 
\cdot
\begin{bmatrix}
\theta_0    \\
\theta_1    \\
\theta_2    \\
\vdots      \\
\theta_n    \\
\end{bmatrix} 
=
\begin{bmatrix}
y_1     \\
y_2     \\
\vdots      \\
y_m     \\
\end{bmatrix} %]]></script>

<p><span>为了不失一般性和简化公式，我们令$x_{i0}|_{i \in (1,2,\cdots,m)} = 1$</span></p>

<p>我们需要找到一个假设$h$，使得应用到上述数据，其代价函数最小。</p>

<script type="math/tex; mode=display">h_{\theta}(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n</script>

<p>或者</p>

<script type="math/tex; mode=display">% <![CDATA[
h_{\theta}(x) = \sum_{j=0}^n\theta_jx_j
= 
\begin{bmatrix}
\theta_0 & \theta_1 & \cdots & \theta_n
\end{bmatrix}
\cdot
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_n \\
\end{bmatrix}
= \theta^TX %]]></script>

<p>其中，$x_0 = 1$, $\theta_0$为偏置。</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2</script>

<p>则$J(\theta)$梯度为</p>

<script type="math/tex; mode=display">\nabla J(\theta) = 
\begin{bmatrix}
\frac{\partial J(\theta)}{\partial\theta_0} \\
\frac{\partial J(\theta)}{\partial\theta_1} \\
\frac{\partial J(\theta)}{\partial\theta_2} \\
\vdots                              \\ 
\frac{\partial J(\theta)}{\partial\theta_n} \\
\end{bmatrix}</script>

<script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial\theta_j} 
= \frac{\partial}{\partial\theta_j}\left(\frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)^2\right) 
= \frac{1}{m}\sum_{i=1}^m\left(h_{\theta}\left(x^{(i)}\right) - y^{(i)}\right)x_j^{(i)}</script>

<p>利用梯度下降法，我们很容易实现对应的迭代解法。</p>

<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)</script>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">sse0</span><span class="p">,</span> <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
                <span class="n">hypothesis</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span> <span class="o">*</span> <span class="n">theta</span><span class="p">[</span><span class="n">jj</span><span class="p">]</span> <span class="k">for</span> <span class="n">jj</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">hypothesis</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss</span>
            <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">m</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">theta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

        <span class="n">sse1</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span> <span class="o">*</span> <span class="n">theta</span><span class="p">[</span><span class="n">jj</span><span class="p">]</span> <span class="k">for</span> <span class="n">jj</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">sse1</span> <span class="o">+=</span> <span class="n">loss</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">sse1</span> <span class="o">=</span> <span class="n">sse1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
        
        <span class="k">print</span> <span class="s">"[ Epoch {0} ] theta = {1}, gd = {2}, sse = {3})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">sse1</span><span class="p">)</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sse1</span> <span class="o">-</span> <span class="n">sse0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sse0</span> <span class="o">=</span> <span class="n">sse1</span>
    <span class="k">return</span> <span class="n">theta</span>
</code></pre>
</div>

<p>之所以不在同一个循环里面同时计算梯度和更新$\theta$，是因为正确的做法需要保证$\theta$同时更新。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 取x_0 = 1</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">4.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">5.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">6.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">7.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">8.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">9.</span><span class="p">)]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.199</span><span class="p">,</span> <span class="mf">0.389</span><span class="p">,</span> <span class="mf">0.580</span><span class="p">,</span> <span class="mf">0.783</span><span class="p">,</span> <span class="mf">0.980</span><span class="p">,</span> <span class="mf">1.177</span><span class="p">,</span> <span class="mf">1.380</span><span class="p">,</span> <span class="mf">1.575</span><span class="p">,</span> <span class="mf">1.771</span><span class="p">]</span>

<span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>

<span class="k">print</span> <span class="s">'y = {0} * x + {1}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c"># y = 0.193953964855 * x + 0.01615274985</span>
</code></pre>
</div>

<h3 id="section-4">更优雅的实现</h3>

<p>之前一直有个疑问，不清楚为什么用显卡可以提高优化算法的执行效率，直到我接触到了如下代码：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
<span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
</code></pre>
</div>

<p>利用矩阵运算，代码可以变的相当简洁，而且可以利用显卡多核的优势。</p>

<p>推导过程如下：</p>

<script type="math/tex; mode=display">LOSS 
= 
\begin{bmatrix}
loss_1 \\
loss_2 \\
\vdots \\
loss_m \\
\end{bmatrix}
=
\begin{bmatrix}
h_{\theta}(x_1) - y_1 \\
h_{\theta}(x_2) - y_2 \\
\vdots \\
h_{\theta}(x_m) - y_m \\
\end{bmatrix}
=
\begin{bmatrix}
\sum_{j=0}^nx_{1j}\theta_j \\
\sum_{j=0}^nx_{2j}\theta_j \\
\vdots \\
\sum_{j=0}^nx_{mj}\theta_j \\
\end{bmatrix}
-
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m \\
\end{bmatrix}
=
\begin{bmatrix}
x_{1} \cdot \theta \\
x_{2} \cdot \theta \\
\vdots \\
x_{m} \cdot \theta \\
\end{bmatrix}
-
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m \\
\end{bmatrix}</script>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{bmatrix}
x_{10} & x_{11} & x_{12} & \cdots & x_{1n} \\
x_{20} & x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{m0} & x_{m1} & x_{m2} & \cdots & x_{mn} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_n \\
\end{bmatrix}
-
\begin{bmatrix}
y_0 \\
y_1 \\
\vdots \\
y_m \\
\end{bmatrix}
= X \cdot \theta - Y %]]></script>

<script type="math/tex; mode=display">GRADIENT = 
\begin{bmatrix}
\frac{\partial}{\partial\theta_0}J(\theta) \\
\frac{\partial}{\partial\theta_1}J(\theta) \\
\vdots \\
\frac{\partial}{\partial\theta_n}J(\theta) \\
\end{bmatrix} 
=
\begin{bmatrix}
\sum_{i=1}^m(h_\theta(x_i) - y_i)x_{i0} \\
\sum_{i=1}^m(h_\theta(x_i) - y_i)x_{i1} \\
\vdots \\
\sum_{i=1}^m(h_\theta(x_i) - y_i)x_{in} \\
\end{bmatrix} 
=
\begin{bmatrix}
\sum_{i=1}^mloss_ix_{i0} \\
\sum_{i=1}^mloss_ix_{i1} \\
\vdots \\
\sum_{i=1}^mloss_ix_{in} \\
\end{bmatrix}</script>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{bmatrix}
x_{10} & x_{20} & \cdots & x_{m0} \\
x_{11} & x_{21} & \cdots & x_{m1} \\
\vdots \\
x_{1n} & x_{2n} & \cdots & x_{mn} \\
\end{bmatrix} 
\cdot
\begin{bmatrix}
loss_1 \\
loss_2 \\
\vdots \\
loss_m \\
\end{bmatrix} 
= X^T \cdot LOSS %]]></script>

<p>完整代码如下</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">sse0</span><span class="p">,</span> <span class="n">sse1</span><span class="p">,</span> <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">hypothesis</span> <span class="o">-</span> <span class="n">Y</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>

        <span class="n">loss2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span>
        <span class="n">sse1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">l</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">loss2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">"[ Epoch {0} ] theta = {1}, gradient = {2}, sse = {3})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">sse1</span><span class="p">)</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sse1</span> <span class="o">-</span> <span class="n">sse0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sse0</span> <span class="o">=</span> <span class="n">sse1</span>
    <span class="k">return</span> <span class="n">theta</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># 取x_0 = 1</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">4.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">5.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">6.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">7.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">8.</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">9.</span><span class="p">)]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.199</span><span class="p">,</span> <span class="mf">0.389</span><span class="p">,</span> <span class="mf">0.580</span><span class="p">,</span> <span class="mf">0.783</span><span class="p">,</span> <span class="mf">0.980</span><span class="p">,</span> <span class="mf">1.177</span><span class="p">,</span> <span class="mf">1.380</span><span class="p">,</span> <span class="mf">1.575</span><span class="p">,</span> <span class="mf">1.771</span><span class="p">]</span>

<span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>

<span class="k">print</span> <span class="s">'y = {0} * x + {1}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c"># y = 0.193953964855 * x + 0.01615274985</span>
</code></pre>
</div>

        </article>

        <div class="row-fluid">
            <ul class="list-inline">
                <li><i class="glyphicon glyphicon-folder-open"></i></li>
                 
    
    <li class="tag"><a href="/categories/ml.html">Machine Learning<span>5</span></a></li>
    

            </ul>
            <ul class="list-inline">
                <li><i class="glyphicon glyphicon-tags"></i></li>
                 
    
    <li class="tag"><a href="/tags/ml.html">Machine Learning<span>5</span></a></li>
    

    
    <li class="tag"><a href="/tags/mathematics.html">数学<span>8</span></a></li>
    

    
    <li class="tag"><a href="/tags/algorithm.html">Algorithm<span>10</span></a></li>
    

    
    <li class="tag"><a href="/tags/python.html">Python<span>6</span></a></li>
    

            </ul>
        </div>

        <div class="row-fluid">
            <ul class="pagination pull-right">
                
                <li class="prev"><a href="/2017/09/17/newton-method-derivation.html" title="牛顿法从放弃到入门">&larr; 上一篇</a></li>
                
                <li><a href="http://localhost:4000">Archive</a></li>
                
                <li class="next disabled"><a>没有了</a> 
            </ul>
        </div>
    </div>

    
</div>

<div class="col-md-3">
    <div data-include="/assets/tpls/widgets/tags.tpl"></div>
<div data-include="/assets/tpls/widgets/categories.tpl"></div>

</div>


</div>

<div class="container" data-include="/assets/tpls/footer.tpl"></div>

<div class="modal fade" id="globalModal" tabindex="-1" role="dialog" aria-labelledby="globalModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
        </div>
    </div>
</div>

<script type="text/javascript" src="/assets/javascripts/seajs/sea.js"></script>
<script type="text/javascript" src="/assets/javascripts/firing.js"></script>
<div class="analytics">
    <script type="text/javascript" src="/assets/javascripts/analytics.js"></script>
</div>
</body>
</html>